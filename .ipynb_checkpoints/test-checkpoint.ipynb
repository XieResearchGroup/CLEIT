{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import data\n",
    "import data_config\n",
    "import preprocess_ccle_gdsc_utils\n",
    "import preprocess_xena_utils\n",
    "import module\n",
    "import loss\n",
    "import train\n",
    "from module import AE, VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data has 1270 samples and 19144 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hedi/PycharmProjects/CLRN/data.py:77: DtypeWarning: Columns (20,30,31) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.labeled_data, self.unlabeled_data, self.labeled_test_data = self._load_data()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data has 1697 samples and 18540 features\n",
      "Propagation Start!\n",
      "Preprocessed data has 9808 samples and 563 features\n",
      "Aligned dataframes have 562 features in common\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hedi/PycharmProjects/CLRN/data.py:122: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  [xena_gex_dat, gex_dat.loc[~gex_dat.index.isin(target_samples+mut_only_target_samples), :]])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (575,19144) (563,) (575,19144) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2b0b9171e45b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m data_provider = data.DataProvider(feature_filter='FILE',\n\u001b[0;32m----> 2\u001b[0;31m                                       omics=['gex', 'mut'], scale_fn=data.min_max_scale)\n\u001b[0m",
      "\u001b[0;32m~/PycharmProjects/CLRN/data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, feature_filter, feature_number, propagation, target, scale_fn, omics, random_seed)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabeled_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlabeled_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabeled_test_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_shape_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatched_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_matched_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/CLRN/data.py\u001b[0m in \u001b[0;36m_load_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmut_scaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_max_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxena_mut_dat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             labeled_gex_dat = pd.DataFrame(self.gex_scaler.transform(labeled_gex_dat),\n\u001b[0m\u001b[1;32m    131\u001b[0m                                            index=labeled_gex_dat.index, columns=labeled_gex_dat.columns)\n\u001b[1;32m    132\u001b[0m             unlabeled_gex_dat = pd.DataFrame(self.gex_scaler.transform(unlabeled_gex_dat),\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    387\u001b[0m                         force_all_finite=\"allow-nan\")\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (575,19144) (563,) (575,19144) "
     ]
    }
   ],
   "source": [
    "data_provider = data.DataProvider(feature_filter='FILE',\n",
    "                                      omics=['gex', 'mut'], scale_fn=data.min_max_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(data_provider.labeled_data['target'].columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>339010</th>\n",
       "      <th>A1CF</th>\n",
       "      <th>A2M</th>\n",
       "      <th>AACS</th>\n",
       "      <th>AACT</th>\n",
       "      <th>AADAC</th>\n",
       "      <th>AADAT</th>\n",
       "      <th>AAK1</th>\n",
       "      <th>AAMP</th>\n",
       "      <th>AANAT</th>\n",
       "      <th>...</th>\n",
       "      <th>ZP3</th>\n",
       "      <th>ZPR1</th>\n",
       "      <th>ZRANB2</th>\n",
       "      <th>ZRANB3</th>\n",
       "      <th>ZRSR2</th>\n",
       "      <th>ZW10</th>\n",
       "      <th>ZWILCH</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZYX</th>\n",
       "      <th>ZZEF1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tumor_Sample_Barcode</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DMS53</th>\n",
       "      <td>0.084050</td>\n",
       "      <td>0.036728</td>\n",
       "      <td>0.050599</td>\n",
       "      <td>0.037589</td>\n",
       "      <td>0.091245</td>\n",
       "      <td>0.034606</td>\n",
       "      <td>0.033245</td>\n",
       "      <td>0.061815</td>\n",
       "      <td>0.035345</td>\n",
       "      <td>0.036301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036996</td>\n",
       "      <td>0.068708</td>\n",
       "      <td>0.034928</td>\n",
       "      <td>0.046363</td>\n",
       "      <td>0.033709</td>\n",
       "      <td>0.037035</td>\n",
       "      <td>0.035312</td>\n",
       "      <td>0.043603</td>\n",
       "      <td>0.433663</td>\n",
       "      <td>0.039922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NCIH1944</th>\n",
       "      <td>0.068229</td>\n",
       "      <td>0.027592</td>\n",
       "      <td>0.034166</td>\n",
       "      <td>0.029766</td>\n",
       "      <td>0.061374</td>\n",
       "      <td>0.031191</td>\n",
       "      <td>0.042489</td>\n",
       "      <td>0.039333</td>\n",
       "      <td>0.026685</td>\n",
       "      <td>0.021742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023532</td>\n",
       "      <td>0.050220</td>\n",
       "      <td>0.023503</td>\n",
       "      <td>0.033103</td>\n",
       "      <td>0.029468</td>\n",
       "      <td>0.026157</td>\n",
       "      <td>0.024782</td>\n",
       "      <td>0.027015</td>\n",
       "      <td>0.035590</td>\n",
       "      <td>0.022365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LS1034</th>\n",
       "      <td>0.049590</td>\n",
       "      <td>0.018423</td>\n",
       "      <td>0.026915</td>\n",
       "      <td>0.023341</td>\n",
       "      <td>0.042628</td>\n",
       "      <td>0.018728</td>\n",
       "      <td>0.018761</td>\n",
       "      <td>0.031340</td>\n",
       "      <td>0.017437</td>\n",
       "      <td>0.019650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025648</td>\n",
       "      <td>0.036292</td>\n",
       "      <td>0.024857</td>\n",
       "      <td>0.031566</td>\n",
       "      <td>0.019601</td>\n",
       "      <td>0.018707</td>\n",
       "      <td>0.018354</td>\n",
       "      <td>0.026716</td>\n",
       "      <td>0.028396</td>\n",
       "      <td>0.019023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H4</th>\n",
       "      <td>0.038297</td>\n",
       "      <td>0.020056</td>\n",
       "      <td>0.021308</td>\n",
       "      <td>0.017415</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.017831</td>\n",
       "      <td>0.020138</td>\n",
       "      <td>0.023031</td>\n",
       "      <td>0.015482</td>\n",
       "      <td>0.015430</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018106</td>\n",
       "      <td>0.031272</td>\n",
       "      <td>0.015381</td>\n",
       "      <td>0.026789</td>\n",
       "      <td>0.014040</td>\n",
       "      <td>0.018348</td>\n",
       "      <td>0.017541</td>\n",
       "      <td>0.017389</td>\n",
       "      <td>0.023210</td>\n",
       "      <td>0.017630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GCIY</th>\n",
       "      <td>0.032154</td>\n",
       "      <td>0.009906</td>\n",
       "      <td>0.021889</td>\n",
       "      <td>0.016974</td>\n",
       "      <td>0.029691</td>\n",
       "      <td>0.011160</td>\n",
       "      <td>0.017268</td>\n",
       "      <td>0.020667</td>\n",
       "      <td>0.017990</td>\n",
       "      <td>0.013660</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010588</td>\n",
       "      <td>0.030332</td>\n",
       "      <td>0.016534</td>\n",
       "      <td>0.020010</td>\n",
       "      <td>0.013857</td>\n",
       "      <td>0.014165</td>\n",
       "      <td>0.017275</td>\n",
       "      <td>0.014763</td>\n",
       "      <td>0.018381</td>\n",
       "      <td>0.017010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        339010      A1CF       A2M      AACS      AACT  \\\n",
       "Tumor_Sample_Barcode                                                     \n",
       "DMS53                 0.084050  0.036728  0.050599  0.037589  0.091245   \n",
       "NCIH1944              0.068229  0.027592  0.034166  0.029766  0.061374   \n",
       "LS1034                0.049590  0.018423  0.026915  0.023341  0.042628   \n",
       "H4                    0.038297  0.020056  0.021308  0.017415  0.035714   \n",
       "GCIY                  0.032154  0.009906  0.021889  0.016974  0.029691   \n",
       "\n",
       "                         AADAC     AADAT      AAK1      AAMP     AANAT  ...  \\\n",
       "Tumor_Sample_Barcode                                                    ...   \n",
       "DMS53                 0.034606  0.033245  0.061815  0.035345  0.036301  ...   \n",
       "NCIH1944              0.031191  0.042489  0.039333  0.026685  0.021742  ...   \n",
       "LS1034                0.018728  0.018761  0.031340  0.017437  0.019650  ...   \n",
       "H4                    0.017831  0.020138  0.023031  0.015482  0.015430  ...   \n",
       "GCIY                  0.011160  0.017268  0.020667  0.017990  0.013660  ...   \n",
       "\n",
       "                           ZP3      ZPR1    ZRANB2    ZRANB3     ZRSR2  \\\n",
       "Tumor_Sample_Barcode                                                     \n",
       "DMS53                 0.036996  0.068708  0.034928  0.046363  0.033709   \n",
       "NCIH1944              0.023532  0.050220  0.023503  0.033103  0.029468   \n",
       "LS1034                0.025648  0.036292  0.024857  0.031566  0.019601   \n",
       "H4                    0.018106  0.031272  0.015381  0.026789  0.014040   \n",
       "GCIY                  0.010588  0.030332  0.016534  0.020010  0.013857   \n",
       "\n",
       "                          ZW10    ZWILCH     ZWINT       ZYX     ZZEF1  \n",
       "Tumor_Sample_Barcode                                                    \n",
       "DMS53                 0.037035  0.035312  0.043603  0.433663  0.039922  \n",
       "NCIH1944              0.026157  0.024782  0.027015  0.035590  0.022365  \n",
       "LS1034                0.018707  0.018354  0.026716  0.028396  0.019023  \n",
       "H4                    0.018348  0.017541  0.017389  0.023210  0.017630  \n",
       "GCIY                  0.014165  0.017275  0.014763  0.018381  0.017010  \n",
       "\n",
       "[5 rows x 10000 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_provider.labeled_data['mut'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x13f8b6a90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(data_provider.labeled_data['target'].isna().sum(), bins=100, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((data_provider.labeled_data['gex'].iloc[train_index].values, data_provider.labeled_data['target'].iloc[train_index].values))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "batch_size = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((data_provider.unlabeled_data['gex'].values, data_provider.unlabeled_data['gex'].values))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((data_provider.labeled_data['gex'].values, data_provider.labeled_data['gex'].values))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gex_auto_encoder = VAE(latent_dim=128, \n",
    "                      output_dim=data_provider.shape_dict['gex'], \n",
    "                      architecture=[1024, 512, 256], \n",
    "                      noise_fn=keras.layers.GaussianNoise, \n",
    "                      output_act_fn=keras.activations.sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x150afe210>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gex_auto_encoder.encoder.load_weights('saved_weights/1024_512_256_128_encoder_weights/pre_trained_gex_encoder_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "WARNING:tensorflow:Layer ae is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Training loss (for one batch) at step 100: 591.4434814453125\n",
      "Seen so far: 6400 samples\n",
      "epoch:  1\n",
      "Training loss (for one batch) at step 100: 128.6375274658203\n",
      "Seen so far: 6400 samples\n",
      "epoch:  2\n",
      "Training loss (for one batch) at step 100: 48.192501068115234\n",
      "Seen so far: 6400 samples\n",
      "epoch:  3\n",
      "Training loss (for one batch) at step 100: 25.877225875854492\n",
      "Seen so far: 6400 samples\n",
      "epoch:  4\n",
      "Training loss (for one batch) at step 100: 18.3756160736084\n",
      "Seen so far: 6400 samples\n",
      "epoch:  5\n",
      "Training loss (for one batch) at step 100: 13.788954734802246\n",
      "Seen so far: 6400 samples\n",
      "epoch:  6\n",
      "Training loss (for one batch) at step 100: 10.676133155822754\n",
      "Seen so far: 6400 samples\n",
      "epoch:  7\n",
      "Training loss (for one batch) at step 100: 9.539257049560547\n",
      "Seen so far: 6400 samples\n",
      "epoch:  8\n",
      "Training loss (for one batch) at step 100: 8.261659622192383\n",
      "Seen so far: 6400 samples\n",
      "epoch:  9\n",
      "Training loss (for one batch) at step 100: 7.039286136627197\n",
      "Seen so far: 6400 samples\n"
     ]
    }
   ],
   "source": [
    "gex_encoder, history_df = train.pre_train_gex_AE(auto_encoder=gex_auto_encoder,train_dataset=train_dataset, val_dataset=val_dataset,max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(module)\n",
    "importlib.reload(loss)\n",
    "importlib.reload(train)\n",
    "from loss import pearson_correlation, penalized_mean_squared_error, spearman_correlation\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_weights/gex/stochastic_1024_512_256_128_encoder_weights exists!\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 1: 0.029921256005764008\n",
      "Training loss (for 5-Fluorouracil) at epoch 1: 0.03193170577287674\n",
      "Training loss (for A-443654) at epoch 1: 0.042609937489032745\n",
      "Training loss (for A-770041) at epoch 1: 0.04203268885612488\n",
      "Training loss (for A-83-01) at epoch 1: 0.04520699381828308\n",
      "Training loss (for ABT737) at epoch 1: 0.043324247002601624\n",
      "Training loss (for ACY-1215) at epoch 1: 0.029156483709812164\n",
      "Training loss (for AGI-5198) at epoch 1: 0.02781117707490921\n",
      "Training loss (for AGI-6780) at epoch 1: 0.021510683000087738\n",
      "Training loss (for AICA Ribonucleotide) at epoch 1: 0.04570828378200531\n",
      "Training loss (total) at epoch 1: 5.428916931152344\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 2: 0.02991814911365509\n",
      "Training loss (for 5-Fluorouracil) at epoch 2: 0.03192724287509918\n",
      "Training loss (for A-443654) at epoch 2: 0.042607083916664124\n",
      "Training loss (for A-770041) at epoch 2: 0.042030006647109985\n",
      "Training loss (for A-83-01) at epoch 2: 0.04520232975482941\n",
      "Training loss (for ABT737) at epoch 2: 0.043321818113327026\n",
      "Training loss (for ACY-1215) at epoch 2: 0.029153898358345032\n",
      "Training loss (for AGI-5198) at epoch 2: 0.027807913720607758\n",
      "Training loss (for AGI-6780) at epoch 2: 0.021509014070034027\n",
      "Training loss (for AICA Ribonucleotide) at epoch 2: 0.0457049161195755\n",
      "Training loss (total) at epoch 2: 5.428691387176514\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 3: 0.029914788901805878\n",
      "Training loss (for 5-Fluorouracil) at epoch 3: 0.03192248195409775\n",
      "Training loss (for A-443654) at epoch 3: 0.042604029178619385\n",
      "Training loss (for A-770041) at epoch 3: 0.04202699661254883\n",
      "Training loss (for A-83-01) at epoch 3: 0.04519739747047424\n",
      "Training loss (for ABT737) at epoch 3: 0.043319106101989746\n",
      "Training loss (for ACY-1215) at epoch 3: 0.02915108948945999\n",
      "Training loss (for AGI-5198) at epoch 3: 0.02780424803495407\n",
      "Training loss (for AGI-6780) at epoch 3: 0.021507173776626587\n",
      "Training loss (for AICA Ribonucleotide) at epoch 3: 0.045701175928115845\n",
      "Training loss (total) at epoch 3: 5.428465843200684\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 4: 0.029911287128925323\n",
      "Training loss (for 5-Fluorouracil) at epoch 4: 0.031917572021484375\n",
      "Training loss (for A-443654) at epoch 4: 0.042600855231285095\n",
      "Training loss (for A-770041) at epoch 4: 0.04202386736869812\n",
      "Training loss (for A-83-01) at epoch 4: 0.04519231617450714\n",
      "Training loss (for ABT737) at epoch 4: 0.04331614077091217\n",
      "Training loss (for ACY-1215) at epoch 4: 0.029148198664188385\n",
      "Training loss (for AGI-5198) at epoch 4: 0.027800463140010834\n",
      "Training loss (for AGI-6780) at epoch 4: 0.021505281329154968\n",
      "Training loss (for AICA Ribonucleotide) at epoch 4: 0.04569743573665619\n",
      "Training loss (total) at epoch 4: 5.428238868713379\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 5: 0.029907718300819397\n",
      "Training loss (for 5-Fluorouracil) at epoch 5: 0.03191259503364563\n",
      "Training loss (for A-443654) at epoch 5: 0.042597606778144836\n",
      "Training loss (for A-770041) at epoch 5: 0.04202073812484741\n",
      "Training loss (for A-83-01) at epoch 5: 0.04518707096576691\n",
      "Training loss (for ABT737) at epoch 5: 0.04331307113170624\n",
      "Training loss (for ACY-1215) at epoch 5: 0.029145196080207825\n",
      "Training loss (for AGI-5198) at epoch 5: 0.027796581387519836\n",
      "Training loss (for AGI-6780) at epoch 5: 0.02150329202413559\n",
      "Training loss (for AICA Ribonucleotide) at epoch 5: 0.0456935316324234\n",
      "Training loss (total) at epoch 5: 5.428011417388916\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 6: 0.0299040749669075\n",
      "Training loss (for 5-Fluorouracil) at epoch 6: 0.03190748393535614\n",
      "Training loss (for A-443654) at epoch 6: 0.04259427636861801\n",
      "Training loss (for A-770041) at epoch 6: 0.042017459869384766\n",
      "Training loss (for A-83-01) at epoch 6: 0.04518182575702667\n",
      "Training loss (for ABT737) at epoch 6: 0.043310075998306274\n",
      "Training loss (for ACY-1215) at epoch 6: 0.029142118990421295\n",
      "Training loss (for AGI-5198) at epoch 6: 0.027792632579803467\n",
      "Training loss (for AGI-6780) at epoch 6: 0.021501250565052032\n",
      "Training loss (for AICA Ribonucleotide) at epoch 6: 0.04568958282470703\n",
      "Training loss (total) at epoch 6: 5.4277849197387695\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 7: 0.029900409281253815\n",
      "Training loss (for 5-Fluorouracil) at epoch 7: 0.03190234303474426\n",
      "Training loss (for A-443654) at epoch 7: 0.04259093105792999\n",
      "Training loss (for A-770041) at epoch 7: 0.04201421141624451\n",
      "Training loss (for A-83-01) at epoch 7: 0.0451764315366745\n",
      "Training loss (for ABT737) at epoch 7: 0.043306827545166016\n",
      "Training loss (for ACY-1215) at epoch 7: 0.029139012098312378\n",
      "Training loss (for AGI-5198) at epoch 7: 0.027788668870925903\n",
      "Training loss (for AGI-6780) at epoch 7: 0.02149919420480728\n",
      "Training loss (for AICA Ribonucleotide) at epoch 7: 0.04568558931350708\n",
      "Training loss (total) at epoch 7: 5.427557468414307\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 8: 0.029896661639213562\n",
      "Training loss (for 5-Fluorouracil) at epoch 8: 0.0318971648812294\n",
      "Training loss (for A-443654) at epoch 8: 0.042587511241436005\n",
      "Training loss (for A-770041) at epoch 8: 0.04201096296310425\n",
      "Training loss (for A-83-01) at epoch 8: 0.0451710969209671\n",
      "Training loss (for ABT737) at epoch 8: 0.043303683400154114\n",
      "Training loss (for ACY-1215) at epoch 8: 0.029135875403881073\n",
      "Training loss (for AGI-5198) at epoch 8: 0.027784645557403564\n",
      "Training loss (for AGI-6780) at epoch 8: 0.021497100591659546\n",
      "Training loss (for AICA Ribonucleotide) at epoch 8: 0.04568147659301758\n",
      "Training loss (total) at epoch 8: 5.427330017089844\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 9: 0.02989291399717331\n",
      "Training loss (for 5-Fluorouracil) at epoch 9: 0.03189196437597275\n",
      "Training loss (for A-443654) at epoch 9: 0.042584098875522614\n",
      "Training loss (for A-770041) at epoch 9: 0.04200759530067444\n",
      "Training loss (for A-83-01) at epoch 9: 0.04516565799713135\n",
      "Training loss (for ABT737) at epoch 9: 0.04330049455165863\n",
      "Training loss (for ACY-1215) at epoch 9: 0.029132694005966187\n",
      "Training loss (for AGI-5198) at epoch 9: 0.027780577540397644\n",
      "Training loss (for AGI-6780) at epoch 9: 0.02149496227502823\n",
      "Training loss (for AICA Ribonucleotide) at epoch 9: 0.04567737877368927\n",
      "Training loss (total) at epoch 9: 5.427103519439697\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 10: 0.02988913655281067\n",
      "Training loss (for 5-Fluorouracil) at epoch 10: 0.03188665956258774\n",
      "Training loss (for A-443654) at epoch 10: 0.042580656707286835\n",
      "Training loss (for A-770041) at epoch 10: 0.04200419783592224\n",
      "Training loss (for A-83-01) at epoch 10: 0.0451602041721344\n",
      "Training loss (for ABT737) at epoch 10: 0.043297260999679565\n",
      "Training loss (for ACY-1215) at epoch 10: 0.029129520058631897\n",
      "Training loss (for AGI-5198) at epoch 10: 0.02777649462223053\n",
      "Training loss (for AGI-6780) at epoch 10: 0.02149283140897751\n",
      "Training loss (for AICA Ribonucleotide) at epoch 10: 0.04567326605319977\n",
      "Training loss (total) at epoch 10: 5.426875114440918\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 11: 0.02988535910844803\n",
      "Training loss (for 5-Fluorouracil) at epoch 11: 0.0318814218044281\n",
      "Training loss (for A-443654) at epoch 11: 0.04257717728614807\n",
      "Training loss (for A-770041) at epoch 11: 0.042000800371170044\n",
      "Training loss (for A-83-01) at epoch 11: 0.04515470564365387\n",
      "Training loss (for ABT737) at epoch 11: 0.04329393804073334\n",
      "Training loss (for ACY-1215) at epoch 11: 0.029126286506652832\n",
      "Training loss (for AGI-5198) at epoch 11: 0.02777237445116043\n",
      "Training loss (for AGI-6780) at epoch 11: 0.0214906707406044\n",
      "Training loss (for AICA Ribonucleotide) at epoch 11: 0.04566916823387146\n",
      "Training loss (total) at epoch 11: 5.426648139953613\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 12: 0.029881536960601807\n",
      "Training loss (for 5-Fluorouracil) at epoch 12: 0.03187611699104309\n",
      "Training loss (for A-443654) at epoch 12: 0.0425737127661705\n",
      "Training loss (for A-770041) at epoch 12: 0.04199737310409546\n",
      "Training loss (for A-83-01) at epoch 12: 0.04514928162097931\n",
      "Training loss (for ABT737) at epoch 12: 0.04329065978527069\n",
      "Training loss (for ACY-1215) at epoch 12: 0.029123060405254364\n",
      "Training loss (for AGI-5198) at epoch 12: 0.027768224477767944\n",
      "Training loss (for AGI-6780) at epoch 12: 0.021488510072231293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for AICA Ribonucleotide) at epoch 12: 0.04566498100757599\n",
      "Training loss (total) at epoch 12: 5.42642068862915\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 13: 0.029877692461013794\n",
      "Training loss (for 5-Fluorouracil) at epoch 13: 0.03187078982591629\n",
      "Training loss (for A-443654) at epoch 13: 0.04257021099328995\n",
      "Training loss (for A-770041) at epoch 13: 0.04199400544166565\n",
      "Training loss (for A-83-01) at epoch 13: 0.04514370858669281\n",
      "Training loss (for ABT737) at epoch 13: 0.04328736662864685\n",
      "Training loss (for ACY-1215) at epoch 13: 0.029119782149791718\n",
      "Training loss (for AGI-5198) at epoch 13: 0.027764081954956055\n",
      "Training loss (for AGI-6780) at epoch 13: 0.0214863121509552\n",
      "Training loss (for AICA Ribonucleotide) at epoch 13: 0.045660749077796936\n",
      "Training loss (total) at epoch 13: 5.426194190979004\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 14: 0.02987384796142578\n",
      "Training loss (for 5-Fluorouracil) at epoch 14: 0.0318654328584671\n",
      "Training loss (for A-443654) at epoch 14: 0.0425666868686676\n",
      "Training loss (for A-770041) at epoch 14: 0.04199054837226868\n",
      "Training loss (for A-83-01) at epoch 14: 0.04513821005821228\n",
      "Training loss (for ABT737) at epoch 14: 0.04328395426273346\n",
      "Training loss (for ACY-1215) at epoch 14: 0.029116518795490265\n",
      "Training loss (for AGI-5198) at epoch 14: 0.027759909629821777\n",
      "Training loss (for AGI-6780) at epoch 14: 0.021484114229679108\n",
      "Training loss (for AICA Ribonucleotide) at epoch 14: 0.045656561851501465\n",
      "Training loss (total) at epoch 14: 5.425966262817383\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 15: 0.029869459569454193\n",
      "Training loss (for 5-Fluorouracil) at epoch 15: 0.031858280301094055\n",
      "Training loss (for A-443654) at epoch 15: 0.04256155341863632\n",
      "Training loss (for A-770041) at epoch 15: 0.04198494553565979\n",
      "Training loss (for A-83-01) at epoch 15: 0.04513116180896759\n",
      "Training loss (for ABT737) at epoch 15: 0.043278709053993225\n",
      "Training loss (for ACY-1215) at epoch 15: 0.02911192923784256\n",
      "Training loss (for AGI-5198) at epoch 15: 0.02775457501411438\n",
      "Training loss (for AGI-6780) at epoch 15: 0.021481171250343323\n",
      "Training loss (for AICA Ribonucleotide) at epoch 15: 0.04564890265464783\n",
      "Training loss (total) at epoch 15: 5.425675392150879\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 16: 0.029863692820072174\n",
      "Training loss (for 5-Fluorouracil) at epoch 16: 0.03184933215379715\n",
      "Training loss (for A-443654) at epoch 16: 0.04255276918411255\n",
      "Training loss (for A-770041) at epoch 16: 0.04197463393211365\n",
      "Training loss (for A-83-01) at epoch 16: 0.04512189328670502\n",
      "Training loss (for ABT737) at epoch 16: 0.043269842863082886\n",
      "Training loss (for ACY-1215) at epoch 16: 0.029105834662914276\n",
      "Training loss (for AGI-5198) at epoch 16: 0.027746930718421936\n",
      "Training loss (for AGI-6780) at epoch 16: 0.02147650718688965\n",
      "Training loss (for AICA Ribonucleotide) at epoch 16: 0.04563695192337036\n",
      "Training loss (total) at epoch 16: 5.4251861572265625\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 17: 0.029838688671588898\n",
      "Training loss (for 5-Fluorouracil) at epoch 17: 0.03182106465101242\n",
      "Training loss (for A-443654) at epoch 17: 0.042482055723667145\n",
      "Training loss (for A-770041) at epoch 17: 0.04188704490661621\n",
      "Training loss (for A-83-01) at epoch 17: 0.045090675354003906\n",
      "Training loss (for ABT737) at epoch 17: 0.04322071373462677\n",
      "Training loss (for ACY-1215) at epoch 17: 0.029085256159305573\n",
      "Training loss (for AGI-5198) at epoch 17: 0.02772248536348343\n",
      "Training loss (for AGI-6780) at epoch 17: 0.021455325186252594\n",
      "Training loss (for AICA Ribonucleotide) at epoch 17: 0.045590758323669434\n",
      "Training loss (total) at epoch 17: 5.423603057861328\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ae/encoder/dense_4/kernel:0', 'ae/encoder/dense_4/bias:0'] when minimizing the loss.\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 18: 0.029795043170452118\n",
      "Training loss (for 5-Fluorouracil) at epoch 18: 0.031757354736328125\n",
      "Training loss (for A-443654) at epoch 18: 0.042368948459625244\n",
      "Training loss (for A-770041) at epoch 18: 0.04174181818962097\n",
      "Training loss (for A-83-01) at epoch 18: 0.04502849280834198\n",
      "Training loss (for ABT737) at epoch 18: 0.043129950761795044\n",
      "Training loss (for ACY-1215) at epoch 18: 0.02903926372528076\n",
      "Training loss (for AGI-5198) at epoch 18: 0.027677170932292938\n",
      "Training loss (for AGI-6780) at epoch 18: 0.021415501832962036\n",
      "Training loss (for AICA Ribonucleotide) at epoch 18: 0.04549533128738403\n",
      "Training loss (total) at epoch 18: 5.421560287475586\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ae/encoder/dense_4/kernel:0', 'ae/encoder/dense_4/bias:0'] when minimizing the loss.\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 19: 0.02974110096693039\n",
      "Training loss (for 5-Fluorouracil) at epoch 19: 0.031674712896347046\n",
      "Training loss (for A-443654) at epoch 19: 0.04223019629716873\n",
      "Training loss (for A-770041) at epoch 19: 0.04156315326690674\n",
      "Training loss (for A-83-01) at epoch 19: 0.044949501752853394\n",
      "Training loss (for ABT737) at epoch 19: 0.043015867471694946\n",
      "Training loss (for ACY-1215) at epoch 19: 0.02897959202528\n",
      "Training loss (for AGI-5198) at epoch 19: 0.027620308101177216\n",
      "Training loss (for AGI-6780) at epoch 19: 0.021365098655223846\n",
      "Training loss (for AICA Ribonucleotide) at epoch 19: 0.045372799038887024\n",
      "Training loss (total) at epoch 19: 5.4192070960998535\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ae/encoder/dense_4/kernel:0', 'ae/encoder/dense_4/bias:0'] when minimizing the loss.\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 20: 0.029679767787456512\n",
      "Training loss (for 5-Fluorouracil) at epoch 20: 0.03157912939786911\n",
      "Training loss (for A-443654) at epoch 20: 0.04207417368888855\n",
      "Training loss (for A-770041) at epoch 20: 0.04136058688163757\n",
      "Training loss (for A-83-01) at epoch 20: 0.0448589026927948\n",
      "Training loss (for ABT737) at epoch 20: 0.04288557171821594\n",
      "Training loss (for ACY-1215) at epoch 20: 0.028910137712955475\n",
      "Training loss (for AGI-5198) at epoch 20: 0.027554847300052643\n",
      "Training loss (for AGI-6780) at epoch 20: 0.0213058739900589\n",
      "Training loss (for AICA Ribonucleotide) at epoch 20: 0.04523107409477234\n",
      "Training loss (total) at epoch 20: 5.416623115539551\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ae/encoder/dense_4/kernel:0', 'ae/encoder/dense_4/bias:0'] when minimizing the loss.\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 21: 0.029613032937049866\n",
      "Training loss (for 5-Fluorouracil) at epoch 21: 0.03147336095571518\n",
      "Training loss (for A-443654) at epoch 21: 0.04190368950366974\n",
      "Training loss (for A-770041) at epoch 21: 0.04114055633544922\n",
      "Training loss (for A-83-01) at epoch 21: 0.04475903511047363\n",
      "Training loss (for ABT737) at epoch 21: 0.04274304211139679\n",
      "Training loss (for ACY-1215) at epoch 21: 0.028833456337451935\n",
      "Training loss (for AGI-5198) at epoch 21: 0.027482911944389343\n",
      "Training loss (for AGI-6780) at epoch 21: 0.021240465342998505\n",
      "Training loss (for AICA Ribonucleotide) at epoch 21: 0.04507574439048767\n",
      "Training loss (total) at epoch 21: 5.413858890533447\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ae/encoder/dense_4/kernel:0', 'ae/encoder/dense_4/bias:0'] when minimizing the loss.\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 22: 0.029541924595832825\n",
      "Training loss (for 5-Fluorouracil) at epoch 22: 0.031359970569610596\n",
      "Training loss (for A-443654) at epoch 22: 0.04172193259000778\n",
      "Training loss (for A-770041) at epoch 22: 0.04090768098831177\n",
      "Training loss (for A-83-01) at epoch 22: 0.044651955366134644\n",
      "Training loss (for ABT737) at epoch 22: 0.04259087145328522\n",
      "Training loss (for ACY-1215) at epoch 22: 0.028750941157341003\n",
      "Training loss (for AGI-5198) at epoch 22: 0.02740618586540222\n",
      "Training loss (for AGI-6780) at epoch 22: 0.02117031067609787\n",
      "Training loss (for AICA Ribonucleotide) at epoch 22: 0.04491052031517029\n",
      "Training loss (total) at epoch 22: 5.410953998565674\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ae/encoder/dense_4/kernel:0', 'ae/encoder/dense_4/bias:0'] when minimizing the loss.\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 23: 0.029467694461345673\n",
      "Training loss (for 5-Fluorouracil) at epoch 23: 0.031240828335285187\n",
      "Training loss (for A-443654) at epoch 23: 0.04153146594762802\n",
      "Training loss (for A-770041) at epoch 23: 0.04066476225852966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for A-83-01) at epoch 23: 0.04453913867473602\n",
      "Training loss (for ABT737) at epoch 23: 0.04243120551109314\n",
      "Training loss (for ACY-1215) at epoch 23: 0.028663650155067444\n",
      "Training loss (for AGI-5198) at epoch 23: 0.02732575684785843\n",
      "Training loss (for AGI-6780) at epoch 23: 0.02109629660844803\n",
      "Training loss (for AICA Ribonucleotide) at epoch 23: 0.04473741352558136\n",
      "Training loss (total) at epoch 23: 5.40793514251709\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ae/encoder/dense_4/kernel:0', 'ae/encoder/dense_4/bias:0'] when minimizing the loss.\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 24: 0.02939150482416153\n",
      "Training loss (for 5-Fluorouracil) at epoch 24: 0.03111693263053894\n",
      "Training loss (for A-443654) at epoch 24: 0.041334234178066254\n",
      "Training loss (for A-770041) at epoch 24: 0.04041460156440735\n",
      "Training loss (for A-83-01) at epoch 24: 0.044421881437301636\n",
      "Training loss (for ABT737) at epoch 24: 0.042265549302101135\n",
      "Training loss (for ACY-1215) at epoch 24: 0.028572559356689453\n",
      "Training loss (for AGI-5198) at epoch 24: 0.027242198586463928\n",
      "Training loss (for AGI-6780) at epoch 24: 0.021020330488681793\n",
      "Training loss (for AICA Ribonucleotide) at epoch 24: 0.0445585697889328\n",
      "Training loss (total) at epoch 24: 5.40482234954834\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ae/encoder/dense_4/kernel:0', 'ae/encoder/dense_4/bias:0'] when minimizing the loss.\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 25: 0.029313229024410248\n",
      "Training loss (for 5-Fluorouracil) at epoch 25: 0.030989520251750946\n",
      "Training loss (for A-443654) at epoch 25: 0.041131630539894104\n",
      "Training loss (for A-770041) at epoch 25: 0.04015928506851196\n",
      "Training loss (for A-83-01) at epoch 25: 0.04430161416530609\n",
      "Training loss (for ABT737) at epoch 25: 0.042096927762031555\n",
      "Training loss (for ACY-1215) at epoch 25: 0.02847857028245926\n",
      "Training loss (for AGI-5198) at epoch 25: 0.02715642750263214\n",
      "Training loss (for AGI-6780) at epoch 25: 0.020942211151123047\n",
      "Training loss (for AICA Ribonucleotide) at epoch 25: 0.04437185823917389\n",
      "Training loss (total) at epoch 25: 5.401637077331543\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ae/encoder/dense_4/kernel:0', 'ae/encoder/dense_4/bias:0'] when minimizing the loss.\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 26: 0.02923295646905899\n",
      "Training loss (for 5-Fluorouracil) at epoch 26: 0.030859142541885376\n",
      "Training loss (for A-443654) at epoch 26: 0.04092572629451752\n",
      "Training loss (for A-770041) at epoch 26: 0.039899349212646484\n",
      "Training loss (for A-83-01) at epoch 26: 0.044178858399391174\n",
      "Training loss (for ABT737) at epoch 26: 0.041927680373191833\n",
      "Training loss (for ACY-1215) at epoch 26: 0.028382383286952972\n",
      "Training loss (for AGI-5198) at epoch 26: 0.027069054543972015\n",
      "Training loss (for AGI-6780) at epoch 26: 0.020862556993961334\n",
      "Training loss (for AICA Ribonucleotide) at epoch 26: 0.04417327046394348\n",
      "Training loss (total) at epoch 26: 5.398388862609863\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ae/encoder/dense_4/kernel:0', 'ae/encoder/dense_4/bias:0'] when minimizing the loss.\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 27: 0.029151469469070435\n",
      "Training loss (for 5-Fluorouracil) at epoch 27: 0.030727021396160126\n",
      "Training loss (for A-443654) at epoch 27: 0.04071716219186783\n",
      "Training loss (for A-770041) at epoch 27: 0.03963634371757507\n",
      "Training loss (for A-83-01) at epoch 27: 0.04405434429645538\n",
      "Training loss (for ABT737) at epoch 27: 0.04175618290901184\n",
      "Training loss (for ACY-1215) at epoch 27: 0.028284952044487\n",
      "Training loss (for AGI-5198) at epoch 27: 0.026980601251125336\n",
      "Training loss (for AGI-6780) at epoch 27: 0.020782597362995148\n",
      "Training loss (for AICA Ribonucleotide) at epoch 27: 0.04396668076515198\n",
      "Training loss (total) at epoch 27: 5.395092010498047\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ae/encoder/dense_4/kernel:0', 'ae/encoder/dense_4/bias:0'] when minimizing the loss.\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 28: 0.029068849980831146\n",
      "Training loss (for 5-Fluorouracil) at epoch 28: 0.030592866241931915\n",
      "Training loss (for A-443654) at epoch 28: 0.040506571531295776\n",
      "Training loss (for A-770041) at epoch 28: 0.039371341466903687\n",
      "Training loss (for A-83-01) at epoch 28: 0.043927714228630066\n",
      "Training loss (for ABT737) at epoch 28: 0.0415833443403244\n",
      "Training loss (for ACY-1215) at epoch 28: 0.028185956180095673\n",
      "Training loss (for AGI-5198) at epoch 28: 0.026891104876995087\n",
      "Training loss (for AGI-6780) at epoch 28: 0.020702242851257324\n",
      "Training loss (for AICA Ribonucleotide) at epoch 28: 0.04375496506690979\n",
      "Training loss (total) at epoch 28: 5.391756057739258\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ae/encoder/dense_4/kernel:0', 'ae/encoder/dense_4/bias:0'] when minimizing the loss.\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 29: 0.02898523211479187\n",
      "Training loss (for 5-Fluorouracil) at epoch 29: 0.03045739233493805\n",
      "Training loss (for A-443654) at epoch 29: 0.040296800434589386\n",
      "Training loss (for A-770041) at epoch 29: 0.03910171985626221\n",
      "Training loss (for A-83-01) at epoch 29: 0.043799519538879395\n",
      "Training loss (for ABT737) at epoch 29: 0.04140917956829071\n",
      "Training loss (for ACY-1215) at epoch 29: 0.028085380792617798\n",
      "Training loss (for AGI-5198) at epoch 29: 0.026800714433193207\n",
      "Training loss (for AGI-6780) at epoch 29: 0.020621158182621002\n",
      "Training loss (for AICA Ribonucleotide) at epoch 29: 0.04354110360145569\n",
      "Training loss (total) at epoch 29: 5.38838529586792\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ae/encoder/dense_4/kernel:0', 'ae/encoder/dense_4/bias:0'] when minimizing the loss.\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 30: 0.02890118956565857\n",
      "Training loss (for 5-Fluorouracil) at epoch 30: 0.03032192587852478\n",
      "Training loss (for A-443654) at epoch 30: 0.0400872528553009\n",
      "Training loss (for A-770041) at epoch 30: 0.03882959485054016\n",
      "Training loss (for A-83-01) at epoch 30: 0.0436699241399765\n",
      "Training loss (for ABT737) at epoch 30: 0.04123400151729584\n",
      "Training loss (for ACY-1215) at epoch 30: 0.027984164655208588\n",
      "Training loss (for AGI-5198) at epoch 30: 0.026710115373134613\n",
      "Training loss (for AGI-6780) at epoch 30: 0.02054024487733841\n",
      "Training loss (for AICA Ribonucleotide) at epoch 30: 0.04332490265369415\n",
      "Training loss (total) at epoch 30: 5.38499116897583\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ae/encoder/dense_4/kernel:0', 'ae/encoder/dense_4/bias:0'] when minimizing the loss.\n"
     ]
    }
   ],
   "source": [
    "gex_fine_tune_train_history, gex_fine_tune_validation_history = train.fine_tune_gex_encoder(\n",
    "        gex_encoder,\n",
    "        target_df=data_provider.labeled_data['target'],\n",
    "        raw_X=data_provider.labeled_data['gex'], max_epoch=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<layer.DenseLayer at 0x144441fd0>,\n",
       " <layer.DenseLayer at 0x141a57690>,\n",
       " <layer.DenseLayer at 0x141a33d90>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x13f8ba290>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x141a36690>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gex_encoder.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (data_provider.unlabeled_data['mut'].loc[data_provider.matched_index].values,\n",
    "     data_provider.unlabeled_data['mut'].loc[data_provider.matched_index].values,\n",
    "    data_provider.unlabeled_data['gex'].loc[data_provider.matched_index].values))\n",
    "\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (data_provider.labeled_data['mut'].values,\n",
    "     data_provider.labeled_data['mut'].values,\n",
    "     data_provider.labeled_data['gex'].values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mut_auto_encoder = VAE(latent_dim=128,\n",
    "                        output_dim=data_provider.shape_dict['mut'],\n",
    "                        architecture=[1024, 512, 256],\n",
    "                        noise_fn=keras.layers.GaussianNoise,\n",
    "                        output_act_fn=keras.activations.sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_weights/mut/stochastic_1024_512_256_128_encoder_weights exists!\n",
      "epoch:  0\n",
      "Training loss (for one batch) at step 100: 6.8567891120910645\n",
      "Seen so far: 6400 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hedi/PycharmProjects/CLRN/train.py:332: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  total_val_loss += val_loss_value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1\n",
      "Training loss (for one batch) at step 100: 7.0842437744140625\n",
      "Seen so far: 6400 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hedi/PycharmProjects/CLRN/train.py:332: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  total_val_loss += val_loss_value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2\n",
      "Training loss (for one batch) at step 100: 6.799269199371338\n",
      "Seen so far: 6400 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hedi/PycharmProjects/CLRN/train.py:332: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  total_val_loss += val_loss_value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  3\n",
      "Training loss (for one batch) at step 100: 6.055013656616211\n",
      "Seen so far: 6400 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hedi/PycharmProjects/CLRN/train.py:332: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  total_val_loss += val_loss_value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  4\n",
      "Training loss (for one batch) at step 100: 5.996334552764893\n",
      "Seen so far: 6400 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hedi/PycharmProjects/CLRN/train.py:332: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  total_val_loss += val_loss_value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  5\n",
      "Training loss (for one batch) at step 100: 5.816469192504883\n",
      "Seen so far: 6400 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hedi/PycharmProjects/CLRN/train.py:332: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  total_val_loss += val_loss_value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  6\n",
      "Training loss (for one batch) at step 100: 5.723364353179932\n",
      "Seen so far: 6400 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hedi/PycharmProjects/CLRN/train.py:332: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  total_val_loss += val_loss_value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  7\n",
      "Training loss (for one batch) at step 100: 5.687492370605469\n",
      "Seen so far: 6400 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hedi/PycharmProjects/CLRN/train.py:332: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  total_val_loss += val_loss_value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  8\n",
      "Training loss (for one batch) at step 100: 5.600029468536377\n",
      "Seen so far: 6400 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hedi/PycharmProjects/CLRN/train.py:332: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  total_val_loss += val_loss_value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  9\n",
      "Training loss (for one batch) at step 100: 5.509373188018799\n",
      "Seen so far: 6400 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hedi/PycharmProjects/CLRN/train.py:332: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  total_val_loss += val_loss_value\n"
     ]
    }
   ],
   "source": [
    "mut_encoder, mut_pre_train_history_df = train.pre_train_mut_AE(mut_auto_encoder, reference_encoder=gex_encoder, train_dataset=train_dataset, val_dataset=val_dataset,transmission_loss_fn=loss.mmd_loss, max_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_weights/mut/stochastic_1024_512_256_128_encoder_weights exists!\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 1: 6.776466369628906\n",
      "Training loss (for 5-Fluorouracil) at epoch 1: 6.81217098236084\n",
      "Training loss (for A-443654) at epoch 1: 5.781530857086182\n",
      "Training loss (for A-770041) at epoch 1: 5.791130542755127\n",
      "Training loss (for A-83-01) at epoch 1: 6.764067649841309\n",
      "Training loss (for ABT737) at epoch 1: 6.647227764129639\n",
      "Training loss (for ACY-1215) at epoch 1: 6.784514904022217\n",
      "Training loss (for AGI-5198) at epoch 1: 6.635828018188477\n",
      "Training loss (for AGI-6780) at epoch 1: 6.825000762939453\n",
      "Training loss (for AICA Ribonucleotide) at epoch 1: 6.802081108093262\n",
      "Training loss (total) at epoch 1: 5.458088397979736\n",
      "100\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 2: 6.776463508605957\n",
      "Training loss (for 5-Fluorouracil) at epoch 2: 6.812168598175049\n",
      "Training loss (for A-443654) at epoch 2: 5.781527042388916\n",
      "Training loss (for A-770041) at epoch 2: 5.791125774383545\n",
      "Training loss (for A-83-01) at epoch 2: 6.764064788818359\n",
      "Training loss (for ABT737) at epoch 2: 6.647220611572266\n",
      "Training loss (for ACY-1215) at epoch 2: 6.784511089324951\n",
      "Training loss (for AGI-5198) at epoch 2: 6.635824680328369\n",
      "Training loss (for AGI-6780) at epoch 2: 6.824997425079346\n",
      "Training loss (for AICA Ribonucleotide) at epoch 2: 6.802077293395996\n",
      "Training loss (total) at epoch 2: 5.457862854003906\n",
      "100\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 3: 6.776460647583008\n",
      "Training loss (for 5-Fluorouracil) at epoch 3: 6.812166213989258\n",
      "Training loss (for A-443654) at epoch 3: 5.78152322769165\n",
      "Training loss (for A-770041) at epoch 3: 5.791121006011963\n",
      "Training loss (for A-83-01) at epoch 3: 6.76406192779541\n",
      "Training loss (for ABT737) at epoch 3: 6.647212982177734\n",
      "Training loss (for ACY-1215) at epoch 3: 6.784506797790527\n",
      "Training loss (for AGI-5198) at epoch 3: 6.635821342468262\n",
      "Training loss (for AGI-6780) at epoch 3: 6.824994087219238\n",
      "Training loss (for AICA Ribonucleotide) at epoch 3: 6.8020734786987305\n",
      "Training loss (total) at epoch 3: 5.457638263702393\n",
      "100\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 4: 6.776457786560059\n",
      "Training loss (for 5-Fluorouracil) at epoch 4: 6.812163829803467\n",
      "Training loss (for A-443654) at epoch 4: 5.781518936157227\n",
      "Training loss (for A-770041) at epoch 4: 5.791116237640381\n",
      "Training loss (for A-83-01) at epoch 4: 6.764058589935303\n",
      "Training loss (for ABT737) at epoch 4: 6.647205352783203\n",
      "Training loss (for ACY-1215) at epoch 4: 6.784502983093262\n",
      "Training loss (for AGI-5198) at epoch 4: 6.635818004608154\n",
      "Training loss (for AGI-6780) at epoch 4: 6.824990749359131\n",
      "Training loss (for AICA Ribonucleotide) at epoch 4: 6.802069664001465\n",
      "Training loss (total) at epoch 4: 5.457411766052246\n",
      "100\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 5: 6.776454925537109\n",
      "Training loss (for 5-Fluorouracil) at epoch 5: 6.812160968780518\n",
      "Training loss (for A-443654) at epoch 5: 5.781514644622803\n",
      "Training loss (for A-770041) at epoch 5: 5.791110992431641\n",
      "Training loss (for A-83-01) at epoch 5: 6.764054775238037\n",
      "Training loss (for ABT737) at epoch 5: 6.647197723388672\n",
      "Training loss (for ACY-1215) at epoch 5: 6.78449821472168\n",
      "Training loss (for AGI-5198) at epoch 5: 6.635814189910889\n",
      "Training loss (for AGI-6780) at epoch 5: 6.824987411499023\n",
      "Training loss (for AICA Ribonucleotide) at epoch 5: 6.802065849304199\n",
      "Training loss (total) at epoch 5: 5.457184791564941\n",
      "100\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 6: 6.776451587677002\n",
      "Training loss (for 5-Fluorouracil) at epoch 6: 6.812158107757568\n",
      "Training loss (for A-443654) at epoch 6: 5.781510353088379\n",
      "Training loss (for A-770041) at epoch 6: 5.7911057472229\n",
      "Training loss (for A-83-01) at epoch 6: 6.76405143737793\n",
      "Training loss (for ABT737) at epoch 6: 6.647189617156982\n",
      "Training loss (for ACY-1215) at epoch 6: 6.784493923187256\n",
      "Training loss (for AGI-5198) at epoch 6: 6.635810375213623\n",
      "Training loss (for AGI-6780) at epoch 6: 6.824983596801758\n",
      "Training loss (for AICA Ribonucleotide) at epoch 6: 6.802061557769775\n",
      "Training loss (total) at epoch 6: 5.456958770751953\n",
      "100\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 7: 6.776448726654053\n",
      "Training loss (for 5-Fluorouracil) at epoch 7: 6.812155723571777\n",
      "Training loss (for A-443654) at epoch 7: 5.781506061553955\n",
      "Training loss (for A-770041) at epoch 7: 5.79110050201416\n",
      "Training loss (for A-83-01) at epoch 7: 6.764048099517822\n",
      "Training loss (for ABT737) at epoch 7: 6.647181510925293\n",
      "Training loss (for ACY-1215) at epoch 7: 6.784489631652832\n",
      "Training loss (for AGI-5198) at epoch 7: 6.635806560516357\n",
      "Training loss (for AGI-6780) at epoch 7: 6.82498025894165\n",
      "Training loss (for AICA Ribonucleotide) at epoch 7: 6.802057266235352\n",
      "Training loss (total) at epoch 7: 5.456732273101807\n",
      "100\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 8: 6.776445388793945\n",
      "Training loss (for 5-Fluorouracil) at epoch 8: 6.812152862548828\n",
      "Training loss (for A-443654) at epoch 8: 5.781501770019531\n",
      "Training loss (for A-770041) at epoch 8: 5.79109525680542\n",
      "Training loss (for A-83-01) at epoch 8: 6.764044284820557\n",
      "Training loss (for ABT737) at epoch 8: 6.6471734046936035\n",
      "Training loss (for ACY-1215) at epoch 8: 6.784485340118408\n",
      "Training loss (for AGI-5198) at epoch 8: 6.635802745819092\n",
      "Training loss (for AGI-6780) at epoch 8: 6.824976444244385\n",
      "Training loss (for AICA Ribonucleotide) at epoch 8: 6.802052974700928\n",
      "Training loss (total) at epoch 8: 5.456505298614502\n",
      "100\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 9: 6.776442050933838\n",
      "Training loss (for 5-Fluorouracil) at epoch 9: 6.812150001525879\n",
      "Training loss (for A-443654) at epoch 9: 5.781497001647949\n",
      "Training loss (for A-770041) at epoch 9: 5.79109001159668\n",
      "Training loss (for A-83-01) at epoch 9: 6.764040946960449\n",
      "Training loss (for ABT737) at epoch 9: 6.647165298461914\n",
      "Training loss (for ACY-1215) at epoch 9: 6.784480571746826\n",
      "Training loss (for AGI-5198) at epoch 9: 6.635798931121826\n",
      "Training loss (for AGI-6780) at epoch 9: 6.824972629547119\n",
      "Training loss (for AICA Ribonucleotide) at epoch 9: 6.802048683166504\n",
      "Training loss (total) at epoch 9: 5.4562788009643555\n",
      "100\n",
      "Training loss (for (5Z)-7-Oxozeaenol) at epoch 10: 6.7764387130737305\n",
      "Training loss (for 5-Fluorouracil) at epoch 10: 6.81214714050293\n",
      "Training loss (for A-443654) at epoch 10: 5.781492710113525\n",
      "Training loss (for A-770041) at epoch 10: 5.791084289550781\n",
      "Training loss (for A-83-01) at epoch 10: 6.764037132263184\n",
      "Training loss (for ABT737) at epoch 10: 6.647157192230225\n",
      "Training loss (for ACY-1215) at epoch 10: 6.784475803375244\n",
      "Training loss (for AGI-5198) at epoch 10: 6.6357951164245605\n",
      "Training loss (for AGI-6780) at epoch 10: 6.824969291687012\n",
      "Training loss (for AICA Ribonucleotide) at epoch 10: 6.80204439163208\n",
      "Training loss (total) at epoch 10: 5.456051826477051\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "mut_fine_tune_train_history, mut_fine_tune_validation_history = train.fine_tune_mut_encoder(\n",
    "        mut_encoder,reference_encoder=gex_encoder, max_epoch=10,\n",
    "        target_df=data_provider.labeled_data['target'],\n",
    "        raw_X=data_provider.labeled_data['mut'],raw_reference_X=data_provider.labeled_data['gex'], transmission_loss_fn=loss.contrastive_loss\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_weights/mut/stochastic_1024_512_256_128_encoder_weights exists!\n",
      "epoch:  0\n",
      "Training loss (for one batch) at step 100: 0.5792152881622314\n",
      "Seen so far: 6400 samples\n",
      "epoch:  1\n",
      "Training loss (for one batch) at step 100: 0.5648735165596008\n",
      "Seen so far: 6400 samples\n",
      "epoch:  2\n",
      "Training loss (for one batch) at step 100: 0.4204457402229309\n",
      "Seen so far: 6400 samples\n",
      "epoch:  3\n",
      "Training loss (for one batch) at step 100: 0.4156975746154785\n",
      "Seen so far: 6400 samples\n",
      "epoch:  4\n",
      "Training loss (for one batch) at step 100: 6.293418884277344\n",
      "Seen so far: 6400 samples\n",
      "epoch:  5\n",
      "Training loss (for one batch) at step 100: 0.23904293775558472\n",
      "Seen so far: 6400 samples\n",
      "epoch:  6\n",
      "Training loss (for one batch) at step 100: 0.19365032017230988\n",
      "Seen so far: 6400 samples\n",
      "epoch:  7\n",
      "Training loss (for one batch) at step 100: 0.1294427216053009\n",
      "Seen so far: 6400 samples\n",
      "epoch:  8\n",
      "Training loss (for one batch) at step 100: 0.12434077262878418\n",
      "Seen so far: 6400 samples\n",
      "epoch:  9\n",
      "Training loss (for one batch) at step 100: 6.1607208251953125\n",
      "Seen so far: 6400 samples\n",
      "epoch:  10\n",
      "Training loss (for one batch) at step 100: 0.11163605004549026\n",
      "Seen so far: 6400 samples\n",
      "epoch:  11\n",
      "Training loss (for one batch) at step 100: 0.03040492534637451\n",
      "Seen so far: 6400 samples\n",
      "epoch:  12\n",
      "Training loss (for one batch) at step 100: 0.051625438034534454\n",
      "Seen so far: 6400 samples\n",
      "epoch:  13\n",
      "Training loss (for one batch) at step 100: 0.01801247149705887\n",
      "Seen so far: 6400 samples\n",
      "epoch:  14\n",
      "Training loss (for one batch) at step 100: 6.065098285675049\n",
      "Seen so far: 6400 samples\n",
      "epoch:  15\n",
      "Training loss (for one batch) at step 100: -0.026795342564582825\n",
      "Seen so far: 6400 samples\n",
      "epoch:  16\n",
      "Training loss (for one batch) at step 100: -0.03235110640525818\n",
      "Seen so far: 6400 samples\n",
      "epoch:  17\n",
      "Training loss (for one batch) at step 100: -0.05535493791103363\n",
      "Seen so far: 6400 samples\n",
      "epoch:  18\n",
      "Training loss (for one batch) at step 100: -0.015677988529205322\n",
      "Seen so far: 6400 samples\n",
      "epoch:  19\n",
      "Training loss (for one batch) at step 100: 6.837274551391602\n",
      "Seen so far: 6400 samples\n",
      "epoch:  20\n",
      "Training loss (for one batch) at step 100: -0.06414675712585449\n",
      "Seen so far: 6400 samples\n",
      "epoch:  21\n",
      "Training loss (for one batch) at step 100: -0.0788913145661354\n",
      "Seen so far: 6400 samples\n",
      "epoch:  22\n",
      "Training loss (for one batch) at step 100: -0.03585111349821091\n",
      "Seen so far: 6400 samples\n",
      "epoch:  23\n",
      "Training loss (for one batch) at step 100: -0.051605723798274994\n",
      "Seen so far: 6400 samples\n",
      "epoch:  24\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-f62c689d51fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmut_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmut_pre_train_history_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_train_mut_AE_with_GAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmut_auto_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference_encoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgex_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PycharmProjects/CLRN/train.py\u001b[0m in \u001b[0;36mpre_train_mut_AE_with_GAN\u001b[0;34m(auto_encoder, reference_encoder, train_dataset, val_dataset, alpha, batch_size, optimizer, loss_fn, min_epochs, max_epochs, n_critic, tolerance, diff_threshold)\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0mtotal_val_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference_x_batch_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m             \u001b[0mtotal_train_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;34m\"\"\"Returns a nested structure of `Tensor`s containing the next element.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2467\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IteratorGetNextSync\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2468\u001b[0m         \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2469\u001b[0;31m         \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   2470\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2471\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mut_encoder, mut_pre_train_history_df = train.pre_train_mut_AE_with_GAN(mut_auto_encoder, reference_encoder=gex_encoder, train_dataset=train_dataset, val_dataset=val_dataset, max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'train' from '/Users/hedi/PycharmProjects/CLRN/train.py'>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xena_gex_dat = pd.read_csv(data_config.xena_preprocessed_gex_file + '.csv', index_col=0)\n",
    "xena_mut_dat = pd.read_csv(data_config.xena_preprocessed_mut_file + '_propagated.csv', index_col=0)\n",
    "gex_dat = preprocess_ccle_gdsc_utils.preprocess_ccle_gex_df(file_path=data_config.ccle_gex_file)\n",
    "mut_dat = preprocess_ccle_gdsc_utils.preprocess_ccle_mut(propagation_flag=True,mutation_dat_file=data_config.ccle_mut_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
